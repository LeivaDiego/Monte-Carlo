{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 5: Monte Carlo\n",
    "\n",
    "**Universidad del Valle de Guatemala**  \n",
    "**Facultad de Ingeniería**  \n",
    "**Departamento de Ciencias de la Computación**  \n",
    "**Aprendizaje por Refuerzo** \n",
    "\n",
    "## Integrantes\n",
    "- Diego Leiva - 21752 \n",
    "- Pablo Orellana - 21970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy policy for action selection.\n",
    "\n",
    "    Args:\n",
    "        Q (dict): The state-action value function.\n",
    "        state: The current state.\n",
    "        nA (int): The number of actions.\n",
    "        epsilon (float): The exploration rate.\n",
    "\n",
    "    Returns:\n",
    "        int: The selected action.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(nA)\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_first_visit(num_episodes=10000, \n",
    "                   gamma=1.0, \n",
    "                   epsilon_start=1.0,\n",
    "                   epsilon_end=0.05,\n",
    "                   epsilon_decay=0.7,\n",
    "                   sab=True,\n",
    "                   seed=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo First Visit Control for Blackjack.\n",
    "    A method for estimating the value of each state-action pair in a reinforcement learning environment.\n",
    "\n",
    "    Args:\n",
    "        num_episodes (int): The number of episodes to run.\n",
    "        gamma (float): The discount factor.\n",
    "        epsilon_start (float): The initial exploration rate.\n",
    "        epsilon_end (float): The minimum exploration rate.\n",
    "        epsilon_decay (float): The decay rate of exploration.\n",
    "        sab (bool): Whether to use the standard action space or the simplified action space.\n",
    "        seed (int): The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        policy (dict): The learned policy.\n",
    "        Q (dict): The state-action value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Deterministic seed\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create environment Blackjack\n",
    "    env = gym.make('Blackjack-v1', sab=sab)\n",
    "    nA = env.action_space.n # Number of actions: 0=stick, 1=hit\n",
    "\n",
    "    # Initialize Q-value function\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    returns_sum = defaultdict(lambda: np.zeros(nA)) # Sum of returns\n",
    "    returns_count = defaultdict(lambda: np.zeros(nA, dtype=int)) # Count of returns\n",
    "\n",
    "    \n",
    "    # Policy ε-soft\n",
    "    def get_epsilon(ep):\n",
    "        # Linearly decaying epsilon\n",
    "        T = int(num_episodes * epsilon_decay)\n",
    "\n",
    "        # Check if we are still in the decay period\n",
    "        if ep < T:\n",
    "            return epsilon_start - (epsilon_start - epsilon_end) * (ep / T)\n",
    "        return epsilon_end # Otherwise, return the minimum epsilon\n",
    "    \n",
    "    # Iterate over episodes\n",
    "    for ep in range(num_episodes):\n",
    "        epsilon = get_epsilon(ep)\n",
    "\n",
    "        # Generate an episode\n",
    "        state, _ = env.reset(seed=seed+ep) # Reset the environment\n",
    "        episode = [] # Initialize episode\n",
    "        terminal = False\n",
    "        truncated = False\n",
    "\n",
    "        # Loop until terminal or truncated\n",
    "        while not (terminal or truncated):\n",
    "            action = epsilon_greedy_policy(Q, state, nA, epsilon) # Select action with ε-soft policy\n",
    "            next_state, reward, terminal, truncated, _ = env.step(action) # Take action with the environment\n",
    "            episode.append((state, action, reward)) # Store state, action, reward\n",
    "            state = next_state # Update state\n",
    "\n",
    "        # First visit MC update\n",
    "        G = 0.0 # Initialize return\n",
    "        visited = set() # Track visited state-action pairs\n",
    "        # Iterate over episode in reverse\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t] # Unpack state, action, reward\n",
    "            G = gamma  * G + r # Update return\n",
    "\n",
    "            # Check if state-action pair is visited\n",
    "            if (s, a) not in visited:  \n",
    "                visited.add((s, a)) # Mark state-action pair as visited\n",
    "                returns_count[s][a] += 1 # Increment count of returns\n",
    "                returns_sum[s][a] += G  # Increment sum of returns\n",
    "                Q[s][a] = returns_sum[s][a] / returns_count[s][a] # Update Q-value\n",
    "\n",
    "        # Final Policy definition\n",
    "        policy = {s: int(np.argmax(Q[s])) for s in Q} # Greedy policy based on Q-values\n",
    "        env.close() # Close the environment\n",
    "        return policy, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonteCarlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
